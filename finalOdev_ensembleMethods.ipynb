{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8675842,"sourceType":"datasetVersion","datasetId":5200310}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-18T12:30:09.383266Z","iopub.execute_input":"2024-09-18T12:30:09.383709Z","iopub.status.idle":"2024-09-18T12:30:09.843683Z","shell.execute_reply.started":"2024-09-18T12:30:09.383669Z","shell.execute_reply":"2024-09-18T12:30:09.842386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/air-quality-and-health-impact-dataset/air_quality_health_impact_data.csv')\npd.set_option('max_colwidth', 100)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:30:12.381901Z","iopub.execute_input":"2024-09-18T12:30:12.382693Z","iopub.status.idle":"2024-09-18T12:30:12.470039Z","shell.execute_reply.started":"2024-09-18T12:30:12.382642Z","shell.execute_reply":"2024-09-18T12:30:12.468773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:30:38.734313Z","iopub.execute_input":"2024-09-18T12:30:38.734883Z","iopub.status.idle":"2024-09-18T12:30:39.559365Z","shell.execute_reply.started":"2024-09-18T12:30:38.734839Z","shell.execute_reply":"2024-09-18T12:30:39.558155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Özellikler ve hedef değişkeni belirleme\nX = df.drop(columns=['HealthImpactClass'])\ny = df['HealthImpactClass']\n\n# Veri setini eğitim ve test olarak ayırma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=58)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:30:47.233091Z","iopub.execute_input":"2024-09-18T12:30:47.233704Z","iopub.status.idle":"2024-09-18T12:30:47.253416Z","shell.execute_reply.started":"2024-09-18T12:30:47.233655Z","shell.execute_reply":"2024-09-18T12:30:47.252264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1) Boosting\n\nBoosting, zayıf öğrenicileri (weak learners) güçlü bir tahminciye dönüştürmeyi amaçlayan bir makine öğrenimi topluluk (ensemble) tekniğidir. Boosting, her bir zayıf öğrenicinin hatalarını düzelterek modelin doğruluğunu artırır.\n\nBoosting'in prensipleri şunlardır:\n\n- Adım Adım Öğrenme: Boosting, zayıf öğrenicileri ardışık olarak eğitir. Her yeni model, önceki modellerin hatalarını düzelterek daha iyi tahmin yapmaya çalışır.\n- Ağırlıklandırma: Her bir örneğin hata oranına göre ağırlıklandırılır. Hatalı sınıflandırılmış örnekler daha yüksek ağırlık alır, böylece sonraki model bu örnekleri daha iyi öğrenir.\n- Topluluk Kararı: Nihai tahmin, tüm zayıf öğrenicilerin tahminlerinin ağırlıklı toplamı veya çoğunluk kararı ile belirlenir.\n\n### 1.1 Gradient Boosting\n\nGradient Boosting, zayıf modellerin ardışık olarak eğitilmesiyle güçlü bir model oluşturur. Her bir model, önceki modellerin hatalarını düzelterek öğrenir.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\n# Gradient Boosting modelini oluşturma\ngb_model = GradientBoostingClassifier(random_state=58)\ngb_model.fit(X_train, y_train)\n\n# Tahmin yapma\ny_pred = gb_model.predict(X_test)\n\n# Sonuçları değerlendirme\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:31:22.813062Z","iopub.execute_input":"2024-09-18T12:31:22.813514Z","iopub.status.idle":"2024-09-18T12:31:34.145053Z","shell.execute_reply.started":"2024-09-18T12:31:22.813469Z","shell.execute_reply":"2024-09-18T12:31:34.143707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 AdaBoost\n\nAdaBoost, zayıf öğreniciler (genellikle decision tree'ler) kullanarak ardışık olarak modeller oluşturur ve her adımda hataları düzelterek güçlü bir model oluşturur.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# AdaBoost modelini oluşturma\nab_model = AdaBoostClassifier(random_state=58)\nab_model.fit(X_train, y_train)\n\n# Tahmin yapma\ny_pred = ab_model.predict(X_test)\n\n# Sonuçları değerlendirme\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:33:00.485843Z","iopub.execute_input":"2024-09-18T12:33:00.486358Z","iopub.status.idle":"2024-09-18T12:33:01.133734Z","shell.execute_reply.started":"2024-09-18T12:33:00.486312Z","shell.execute_reply":"2024-09-18T12:33:01.132343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 XGBoost\n\nXGBoost, performans ve hız açısından oldukça güçlü bir boosting algoritmasıdır. Aynı zamanda eksik verilerle başa çıkma yeteneği de vardır.","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# XGBoost modelini oluşturma\nxgb_model = xgb.XGBClassifier(random_state=58)\nxgb_model.fit(X_train, y_train)\n\n# Tahmin yapma\ny_pred = xgb_model.predict(X_test)\n\n# Sonuçları değerlendirme\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:33:32.199029Z","iopub.execute_input":"2024-09-18T12:33:32.199494Z","iopub.status.idle":"2024-09-18T12:33:33.012416Z","shell.execute_reply.started":"2024-09-18T12:33:32.199451Z","shell.execute_reply":"2024-09-18T12:33:33.011153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2) Majority Voting\n\nMajority voting ensemble, birden fazla makine öğrenimi modelinin tahminlerini birleştirerek nihai sınıflandırma kararını veren bir ensemble yöntemidir. Bu yöntemde, her bir modelin verdiği sınıf tahminleri alınır ve en çok oy alan sınıf, nihai tahmin olarak seçilir. Bu yaklaşım, farklı modellerin güçlü yönlerini birleştirerek daha doğru ve sağlam bir tahmin yapmayı amaçlar.\n\nÇalışma Prensibi\nFarklı Modellerin Eğitimi: Farklı türde makine öğrenimi modelleri (örneğin, decision tree, k-nearest neighbors, logistic regression) aynı veri seti üzerinde eğitilir.\nTahminlerin Alınması: Test verileri üzerinde her bir modelin tahminleri yapılır.\nOyların Birleştirilmesi: Her bir veri noktası için modellerin yaptığı tahminler alınır ve en çok oyu alan sınıf, nihai tahmin olarak belirlenir.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Veri setini yükleyin ve bağımlı ve bağımsız değişkenleri ayır\nX = df.drop(columns=['HealthImpactClass'])\ny = df['HealthImpactClass']\n\n# Veri setini eğitim ve test olarak ayır\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=58)\n\n# Modelleri tanımla\nlog_clf = LogisticRegression(random_state=58, max_iter=10000)\nknn_clf = KNeighborsClassifier(n_neighbors=5)\ndt_clf = DecisionTreeClassifier(random_state=58)\n\n# Majority Voting Ensemble oluştur\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('knn', knn_clf), ('dt', dt_clf)],\n    voting='hard'  # 'hard' majority voting, 'soft' probability averaging\n)\n\n# Ansambl modelini eğit\nvoting_clf.fit(X_train, y_train)\n\n# Tahmin yapın\ny_pred = voting_clf.predict(X_test)\n\n# Sonuçları değerlendir\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:34:26.871582Z","iopub.execute_input":"2024-09-18T12:34:26.872117Z","iopub.status.idle":"2024-09-18T12:34:52.75224Z","shell.execute_reply.started":"2024-09-18T12:34:26.872074Z","shell.execute_reply":"2024-09-18T12:34:52.751092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3) Bagging (Bootstrap Aggregating)\n\nBagging yöntemi, modelin genel performansını artırmak amacıyla birden fazla modelin aynı veri setinden rastgele seçilmiş alt örnekler üzerinde eğitildiği bir ensemble yöntemidir. Bagging, overfitting riskini azaltarak modelin genelleme yeteneğini artırır. Bu yöntem, özellikle yüksek varyansa sahip modellerde etkili sonuçlar verir.\nBu yönteme örnek olarak Random Forest’ı ayrı bir .ipynb dosyasında kullandım. Random Forest da bir bagging yöntemidir.","metadata":{}},{"cell_type":"markdown","source":"## 4) Stacking\n\nStacking, birden fazla modelin tahminlerini bir araya getirerek, nihai tahmini yapmak için bu tahminleri ikinci bir modelle (meta-learner) öğrenen bir ensemble yöntemidir. Stacking, temel öğrenicilerin (base learners) zayıf yanlarını telafi ederek genel model performansını artırmayı amaçlar.\n\nStacking'in Adımları\nTemel Öğrenicilerin Eğitimi (Base Learners):\n\nFarklı algoritmalardan bir dizi temel model (örneğin, lojistik regresyon, SVM, karar ağaçları) eğitilir.\nBu modeller veri setinin farklı özelliklerini yakalamaya çalışır.\nMeta Öğrenicinin Eğitimi (Meta-Learner):\n\nTemel öğrenicilerin tahminleri bir araya getirilir ve bu tahminler, yeni bir model (meta-learner) ile birleştirilir.\nMeta-learner, temel öğrenicilerin tahminlerine dayanarak nihai tahmini yapar.\n\nStacking, çok sınıflı veri setlerinde farklı model türlerinin güçlü yönlerini bir araya getirerek daha iyi bir genel performans sağlayabilir. Özellikle bu veri setinde, çeşitli hava kalitesi ve sağlık etkisi ölçümlerine dayanan çok sayıda sınıf bulunduğundan, stacking gibi bir ensemble yöntemi, sınıflandırma performansını artırmada etkili olabilir.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Veri setini yükleme\n# df = pd.read_csv('path_to_dataset.csv')\n\n# Özellikler ve hedef değişkeni belirleme\nX = df.drop(columns=['HealthImpactClass'])\ny = df['HealthImpactClass']\n\n# Veri setini eğitim ve test olarak ayırma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=58)\n\n# Temel öğrenicileri tanımlama\nestimators = [\n    ('lr', LogisticRegression(max_iter=1000, random_state=58)),\n    ('knn', KNeighborsClassifier(n_neighbors=5)),\n    ('dt', DecisionTreeClassifier(random_state=58))\n]\n\n# StackingClassifier'ı tanımlama\nstacking_clf = StackingClassifier(\n    estimators=estimators,\n    final_estimator=LogisticRegression(max_iter=1000, random_state=58),\n    cv=5\n)\n\n# Modeli eğitme\nstacking_clf.fit(X_train, y_train)\n\n# Tahmin yapma\ny_pred = stacking_clf.predict(X_test)\n\n# Sonuçları değerlendirme\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-18T12:36:07.566285Z","iopub.execute_input":"2024-09-18T12:36:07.56681Z","iopub.status.idle":"2024-09-18T12:36:15.80037Z","shell.execute_reply.started":"2024-09-18T12:36:07.566765Z","shell.execute_reply":"2024-09-18T12:36:15.798758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}